\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{youtube_stats}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem statement}{5}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Distributed image processing}{6}{subsection.1.1.1}}
\citation{srirama2012adapting}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Differences between local (left) and non-local (right) processing. Red represents the pixel in focus, the X-s represent the pixels whose data the algorithm needs to access.}}{8}{figure.1.1}}
\newlabel{fig_local_nonlocal}{{1.1}{8}{Differences between local (left) and non-local (right) processing. Red represents the pixel in focus, the X-s represent the pixels whose data the algorithm needs to access}{figure.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Illustration of the data requirements of iterative local processing. The top row represents first local computations of the first iteration. The diagram on the bottom shows the requirements at the start of the second iteration. Arrows encode a 'depends on' relationship: the value from which the arrow originates depends on the value the arrow is pointing at.}}{9}{figure.1.2}}
\newlabel{fig_local_iterative}{{1.2}{9}{Illustration of the data requirements of iterative local processing. The top row represents first local computations of the first iteration. The diagram on the bottom shows the requirements at the start of the second iteration. Arrows encode a 'depends on' relationship: the value from which the arrow originates depends on the value the arrow is pointing at}{figure.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Why use MapReduce?}{10}{subsection.1.1.2}}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{White:2010:WCV:1814245.1814254}
\citation{Pereira:2010:ADH:1844768.1845374}
\citation{website:facebook_namenode_improvements}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{11}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Relevant work}{11}{section.2.1}}
\citation{citeulike:2631015}
\citation{Lv:2010:PKC:1927661.1927687}
\citation{Zhang:2009:CSS:2127968.2128002}
\citation{trease08}
\citation{Hawick:2003:DFP:958021.958024}
\citation{website:amazon_ec2}
\citation{5446706}
\citation{wang2011ult}
\citation{Dean:2008:MSD:1327452.1327492}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}MapReduce}{14}{section.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A simple example of the MapReduce computation model, inspired by the WordCount example provided in the Apache Hadoop Getting Started tutorial. A text file is first converted into pairs of line number and its content (Input), then the Map function splits these pairs further so that the reducer receives one pair per occurrence of a word. The objective of the Reduce function is then to count the individual occurrences and finally output the total per each distinct word.}}{15}{figure.2.1}}
\newlabel{fig_mapreduce}{{2.1}{15}{A simple example of the MapReduce computation model, inspired by the WordCount example provided in the Apache Hadoop Getting Started tutorial. A text file is first converted into pairs of line number and its content (Input), then the Map function splits these pairs further so that the reducer receives one pair per occurrence of a word. The objective of the Reduce function is then to count the individual occurrences and finally output the total per each distinct word}{figure.2.1}{}}
\citation{white2012hadoop}
\citation{ghemawat2003google}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Apache Hadoop}{16}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{Hadoop Distributed Filesystem (HDFS)}{16}{section*.2}}
\citation{imagej}
\@writefile{toc}{\contentsline {subsubsection}{Running a MapReduce job}{17}{section*.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Image processing}{17}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Bilateral filter}{18}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{Gaussian blur}{18}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}}{18}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Technologies used}{18}{subsection.2.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{ImageJ}{18}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{OpenCV}{18}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{Tesseract}{18}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{Pandore}{18}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{ImageMagick}{18}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{Metadata-extractor}{18}{section*.10}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Image processing and MapReduce}{18}{section.2.4}}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{chaudhury2011fast}
\citation{aurich1995non,smith1997susan,tomasi1998bilateral}
\citation{bf_course}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Use case}{20}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Processing a large image using a local non-iterative algorithm}{20}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Description of the data and use case}{20}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Bilateral Filter}{21}{subsection.3.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces TODO pretty picture of gaussian blur}}{22}{figure.3.1}}
\newlabel{fig_gaussian_blur}{{3.1}{22}{TODO pretty picture of gaussian blur\relax }{figure.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Naive bilateral filter algorithm}{22}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces TODO pretty picture of bilateral filter}}{23}{figure.3.2}}
\newlabel{fig_gaussian_blur}{{3.2}{23}{TODO pretty picture of bilateral filter\relax }{figure.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fast O(1) bilateral filter algorithm}{23}{section*.12}}
\@writefile{toc}{\contentsline {subsubsection}{}{23}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Performance}{23}{subsection.3.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Analysis of many small images}{24}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Description of the data and possible use case}{24}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Problem statement}{24}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Performance}{25}{subsection.3.2.3}}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Overview and discussion}{26}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{bibliography}
\bibcite{citeulike:2631015}{1}
\bibcite{aurich1995non}{2}
\bibcite{Dean:2008:MSD:1327452.1327492}{3}
\bibcite{website:facebook_namenode_improvements}{4}
\bibcite{website:amazon_ec2}{5}
\bibcite{trease08}{6}
\bibcite{Hawick:2003:DFP:958021.958024}{7}
\bibcite{Lv:2010:PKC:1927661.1927687}{8}
\bibcite{bf_course}{9}
\bibcite{Pereira:2010:ADH:1844768.1845374}{10}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{27}{chapter.4}}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{5446706}{11}
\bibcite{imagej}{12}
\bibcite{smith1997susan}{13}
\bibcite{srirama2012adapting}{14}
\bibcite{tomasi1998bilateral}{15}
\bibcite{wang2011ult}{16}
\bibcite{White:2010:WCV:1814245.1814254}{17}
\bibcite{youtube_stats}{18}
\bibcite{Zhang:2009:CSS:2127968.2128002}{19}
