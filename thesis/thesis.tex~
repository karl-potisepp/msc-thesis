% TÜ Arvutiteaduse instituudi lõputöö eeskuju ver.2.0
% Tõnu Tamme, 2011
% Jüri Kiho, 2003
% todo: joonised, leheküljemõõdud, numbripunktid, mac, lyx
% iconv -f ISO_8859-15 -t UTF-8 -o eeskuju_utf8.tex eeskuju.tex
  %preambula ---------------------------------------------------
  \documentclass [12pt,a4paper]{report}
  \newif\ifutf
  \utftrue %lülitab UTF-8 sisse/välja (\utftrue|\utffalse)
  \usepackage[T1]{fontenc} %täpitäht ühe sümbolina
  \ifutf
  \usepackage[utf8]{inputenc} %sisendfaili koodilehekylg UTF-8
  \usepackage{ut_thesis_utf8}% ver.0.6
  \else
  \usepackage[latin1]{inputenc} %sisendfaili koodilehekylg ISO 8859-1
  \usepackage{ut_thesis}% ver.0.6  
  \fi
  \usepackage[english]{babel}
  %\usepackage{times} %Times New Roman
  \usepackage{indentfirst} %esimene lõik taandega
  \usepackage{graphicx} %joonised failidena
  \usepackage{algorithm2e}
  \usepackage{multirow}
%  \usepackage{pdfcomment}
  \newcommand{\pdfcomment}[1]{}
  \newcommand{\comment}[1]{}
%\usepackage[margin=1in]{geometry}
%\usepackage[showframe]{geometry}
  %\usepackage{url} %\url
  \usepackage{hyperref} %\url+lingid
  \renewcommand\url{\begingroup\urlstyle{rm}\Url} 
  \usepackage{amssymb,amsmath,amsthm}
  \renewcommand{\leq}{\leqslant} %väiksem-või-võrdne märk eriti ilusasti
  \renewcommand{\geq}{\geqslant} %suurem-või-võrdne märk eriti ilusasti
\newcommand{\TM}{$^{\mathrm{tm}}$ }
  \sloppy
  % ------------------------------------------------------------

%\documentclass{report}
%\usepackage{hyperref}
\usepackage{cite}
%\author{Karl Potisepp, Pelle Jakovits, Satish Narayana Srirama}
%\date{\today}
%\title{Large-scale image processing using MapReduce}

\begin{document}

\begin{tiitelleht}
\pealkiri{Large-scale image processing using MapReduce}
\paber{M.Sc. Thesis (30 ECTS)}
\autor{Karl Potisepp}

\teaduskond{Faculty of Mathematics and Computer Science}
\instituut{Institute of Computer Science}
\eriala{Computer Science}

\juhendaja{Pelle Jakovits M.Sc., Satish Narayana Srirama Ph.D.}

\end{tiitelleht}

%\maketitle

\tableofcontents

\chapter{Introduction}

Along with the development of information technology, a constant stream of new applications for solving humanity's problems has also appeared. As we possess more computing power, we can tackle more and more resource-intensive problems such as DNA sequencing, seismic imaging and weather simulations. When looking at these subjects, a common theme emerges: all of these involve either analysis or generation of large amounts of data. While personal computers have gone through a staggering increase in power during the last 20 years, and the processing power even within everyday accessories - such as smartphones - is very capable of solving problems that were unfeasible for supercomputers only a couple of decades ago, analysing the amount of data generated by newest generation scientific equipment is still out of reach in some areas. Moreover, as processor architectures are reaching their physical limitations (TODO explain this in more detail?), using distributed computing technologies has become a popular way to solve problems which do not fit the confines of a single computer. Broadly speaking, supercomputers, GRID-based systems and computing clouds are an example of this approach. In this work I will focus on the latter of the three with regard to image processing.

In this thesis, I will describe solving two different image processing tasks related to two-dimensional colour images using the Hadoop open source distributed computing framework. %TODO


Description in broad terms of what has been achieved with this thesis.

Possible subtopics:
\begin{itemize}
	\item Satellite images processing - \url{http://en.wikipedia.org/wiki/Interferometric_synthetic_aperture_radar} (Mobile Cloud lab, U of Tartu)
	\item Cancer scan analysis for tumor detection - Datexim SAS (ENSICAEN)
	\item Classification and metadata extraction from archaeological excavation data (TO BE ELABORATED)- Graeme Earl and Hembo Pagi (U of Southampton)
	
\end{itemize}

\section{Problem statement}

Processing of large images can not be done on a single personal computer, because they do not fit into memory. Processing of many small images also requires distributed computing because it takes too much time. Neither of these issues will reliably be solved by advances in computing power, because CPUs are already reaching their theoretical physical limitations and the scale of data is increasing faster than the processing capabilities of a single computer.

A solution for these problems is turning towards distributed computing, where limitations of a single computer are dealt with by combining the resources of many computers to perform one large task. While this approach is not new - supercomputers and computing clusters have existed for many years already - it is only recently that techniques of using commodity computers for distributed processing have gained popularity. Combining this with the ability to rent virtual machines from cloud providers such as Amazon and Microsoft it is easy to see how solving large scale processing tasks is now feasible for those who do not have access to a supercomputer nor the resources to build or maintain one.

\subsection{Image processing}

Lots of images on the internet, youtube, produced by high definition cameras. Realtime processing and feature extraction, denoising. Focus on 2-dimensional color images, subfocus on one large image and a dataset of many small images with regard to MapReduce.

\subsection{Why MapReduce?}

Problem statement. What is the problem, why does it need to be solved, why solve it this way?

\chapter{Background}

\section{Relevant work}
Information on how others have solved similar problems (if any), and what are the differences in approach and why the methodology described in this work is different enough to warrant attention.

\begin{itemize}
	\item Web-Scale Computer Vision using MapReduce for Multimedia Data Mining \cite{White:2010:WCV:1814245.1814254}: A case study of classifying and clustering billions of regular images using MapReduce. No mention is made of average image dimensions or any issues with not being able to process certain images because of memory limitations. However, a way of pre-processing images for use in a sliding-window approach for object recognition is described. Therefore one can assume that in this approach, the size of images is not an issue, because the pre-processing phase cuts everything into a manageable size. The question still remains whether a sliding window approach is capable of recognizing any objects present in the image that do not easily fit into one analysis window, and whether the resource requirements for image classification and image processing are significantly different or not.
	\item An Architecture for Distributed High Performance Video Processing in the Cloud \cite{Pereira:2010:ADH:1844768.1845374}: This article outlines some of the limitations of the MapReduce model when dealing with high-speed video encoding, namely it's dependence on the NameNode as a single point of failure (however a fix is claimed at \cite{website:facebook_namenode_improvements}), and lack of possibility for generalization in order to suit the issue at hand. An alternative - optimized - implementation is proposed for providing a cloud-based IaaS solution. However, considering the advances of distributed computation technology within the past two years (the article was published in 2010) and the fact that the processing of truly large images was not touched upon, the problem posed in this work still remains.
	\item Clustering Billions of Images with Large Scale Nearest Neighbor Search \cite{citeulike:2631015}: Here, a MapReduce-based approach for parallelizing nearest-neighbor clustering is described. The report focuses more on the technicalities of adapting a spill-tree based approach for use on multiple machines. Also, a way for compressing image information into smaller feature vectors is described. With regards to this thesis, again the focus is not so much on processing the images to attain some other result than something intermediate to be used in search and clustering.
	\item Parallel K-Means Clustering of Remote Sensing Images Based on MapReduce \cite{Lv:2010:PKC:1927661.1927687}: A description of sing the k-means algorithm in conjunction with MapReduce and satellite/aerophoto images in order to find different elements based on their color (i.e. separate trees from buildings). Not much is told about encountering and overcoming the issues of analyzing large images besides mentioning that a non-parallel approach was unable to process images larger than 1000x1000 pixels, and that the use of a MapReduce-based parallel processor required the conversion of TIFF files into a plaintext format. 
	\item Case Study of Scientific Data Processing on a Cloud Using Hadoop \cite{Zhang:2009:CSS:2127968.2128002}: This article describes the methods used for processing sequences of microscope images of live cells. The images and data units in question are relatively small - 512x512 16-bit pixels, stored in folders measuring 90MB - there were no issues with regard to having to split data for fitting into Hadoop DFS chunks besides improving upon a couple of Java classes. No mention was made about the algorithm used to extract data from the images besides that it was written in MATLAB, which meant that the processing chain had much more complexity than just a MapReduce job.
	\item Using Transaction Based Parallel Computing to Solve Image Processing and Computational Physics Problems \cite{trease08}: This article describes the use of distributed computing with two examples - video processing/analysis and subsurface transport. The main focus is put on the specifications of the technology used (Apache Hadoop, PNNL MeDICI), whereas there is no information presented on how the image processing parts of the examples given were implemented.
	\item Distributed frameworks and parallel algorithms for processing large-scale geographic data \cite{Hawick:2003:DFP:958021.958024}: Here, many issues with processing large sets of geographic information systems (commonly known as GIS) data are described and solved in order to be able to extract useful knowledge from it. This article was published in 2003, so while some of the issues have disappeared due to the increase in computing power available to scientists, problems stemming from the ever-increasing amount of data generated by different types of monitoring technologies (such as ensuring distribution of data to computation nodes and storing big chunks of data in memory) still remain. Also, considering that the Amazon EC2 \cite{website:amazon_ec2} web service came online just in 2006, it is obvious that one can not make an apt comparison whether or not a MapReduce-based solution in 2012 is better or not for large-scale image processing than what was possible using grid technology in 2003.
	\item A Scalable Image Processing Framework for gigapixel Mars and other celestial body  images \cite{5446706}: This article describes the way NASA handles processing of celestial images captured by the Mars orbiter and rovers. Clear and concise descriptions are provided for the segmentation of gigapixel images into tiles, how these tiles are processed, and how the image processing framework handles scaling and works with distributed processing. The authors used the Kakadu JPEG2000 encoder and decoder along with the Kakadu Java Native Interface to develop their own processing suite. The software is proprietary and requires the purchase of a license to use.
	\item Ultra-fast processing of gigapixel Tissue MicroArray images using high performance computing \cite{wang2011ult}: This article talks about speeding up the analysis of Tissue MicroArray images by substituting human expert analysis for automated processing algorithms. While the images sizes processed were measured in gigapixels, the content of the image (scans of tissue microarrays) was easily segmented and there was no need to focus on being able to analyse all of the image at once. Furthermore, the work was all done on a specially built grid high performance computing platform with shared memory and storage, whereas this thesis is focused on performing processing on a MapReduce cluster.
\end{itemize}

In this chapter I will provide a thorough description of all technologies and methodologies that are important with regards to this work. I will describe the operating philosophy of the MapReduce model and the capabilities and limitations of it's most popular freely non-proprietary implementation - Hadoop. Also, the different image processing and Java-C++ technologies used while working on this thesis, such as the CImg library and the Java Native Interface, will be elaborated.

\section{MapReduce}

MapReduce is a programming model developed by Google for processing and generating large datasets used in practice for many real-world tasks \cite{Dean:2008:MSD:1327452.1327492}. In this section, I will focus on describing the general philosophy and methodology behind this model, whereas the following part will describe in more detail one of the more popular implementations of the model - Hadoop - which is also used for all the practical applications featured in this work.

The basic idea behind MapReduce is based on the observation that a lot of processing tasks involving large amounts of data (measured in terabytes) all need to deal with the issues of distributing the data across a network of computers to ensure that the available storage and CPU resources are maximally utilised, and it would be easier if programmers could focus on writing the actual processing part that differs task by task.

Description of the general philosophy of MapReduce, the logic behind Map/Reduce/etc. parts of the processing chain, and how this approach suits/is cumbersome when dealing with image processing.

\subsection{Hadoop}
More in-depth explanation about how Hadoop as a freely-available and popular MapReduce framework suits the needs of this work.
\subsubsection{HDFS}
what is the Hadoop Distributed File System and how does it restrict image processing using MapReduce.

\section{Image processing}
Description of what is meant by image processing within the scope of this thesis.
\subsection{ImageJ}
Information about the Imagej library. 


\section{Image processing and MapReduce}
Description of different ways the MapReduce method has been used in conjunction with image processing libraries. Also some use cases.

\chapter{Details of methodology}
Descriptions on what was implemented, how it was implemented, problems encountered, and what has been done to solve these problems. If there were unsolvable problems, then how do these limit the applicability of the methodology in the real world and how to work around these limitations.

\chapter{Use case}
Description of how the methodology established in this thesis was applied to real-world problems.

\section{Analysis of a large image}

\subsection{Description of the data and use case}
What sort of image, why does it need processing?

\subsection{Bilateral Filter Algorithm}
Description of the bilateral filter algorithm, it's applications and details about the implementation used for benchmarking.

\section{Analysis of many small images}

\subsection{Description of the data and possible use case}

\subsection{Problem statement}

Extracting meaningful data from images is a non-trivial task. OCR, object recognition, identifying similar images, measuring image quality.


\clearpage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
